{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building configurable neural networks\n",
    "\n",
    "The goal of this part is to get you up and running with the wonderful library from the AllenAI people. The library implements all the details we have coded ourselves (batching, padding, etc.). More importantly it provides a configurable architecture which lets you both experiment and evolve your models.\n",
    "\n",
    "In this case we will be rebuilding our classifier with configurable embeddings (word-level and char-level) and a configurable encoder (BoE, RNNs, CNNs, etc.)\n",
    "\n",
    "In particular, we will be implementing three classes. \n",
    "\n",
    "- The first is a <a href =\"https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.html\">DatasetReader</a>, which contains the logic for reading a file of data and producing a stream of <code>Instance</code>s.\n",
    "- The second is a configurable `Model`, which can combine different modules (seq2vec encoders such as lstms, cnns, Elmo, etc.).\n",
    "\n",
    "But first let's prepare our datasets. This time we will be using the training and validation splits provided by fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "import pandas as pd\n",
    "data_path = fastai.untar_data(fastai.URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(data_path/'texts.csv')\n",
    "train_df = df.loc[df['is_valid'] == False] # get examples from train split\n",
    "validation_df = df.loc[df['is_valid'] == True] # get examples from valid split\n",
    "train_df.to_csv(data_path/'train.csv')\n",
    "validation_df.to_csv(data_path/'validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AllenNLP uses type annotations\n",
    "from typing import Iterator, List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# AllenNLP represent each training example as Instances, containing several fields\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "# Abstract DatasetReader, similar to our previous 'Dataset'\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "# Tokenizer and numericalizers utilities\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDatasetReader(DatasetReader):\n",
    "    #  <code>TokenIndexer</code>s similar to our previous to_id method\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    # This creates and wraps training and evaluating examples\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        text_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"text\": text_field}\n",
    "\n",
    "        if label:\n",
    "            label_field = LabelField(label)\n",
    "            fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    " \n",
    "    # This reads the file and builds instance for each example\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        dataset = pd.read_csv(file_path)\n",
    "        for _, row in dataset.iterrows():\n",
    "            yield self.text_to_instance([Token(word) for word in row['text']],\n",
    "                                       row['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "Our model will be composed of the following layers:\n",
    "\n",
    "- text embeddings\n",
    "- encoder\n",
    "- linear layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder\n",
    "from allennlp.models import Model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "    \n",
    "class TextClassifier(Model): # Inherit from allenNLP model which wraps torch.nn.Module\n",
    "    def __init__(self,\n",
    "                # here we can plug dif embeddings (char, words, elmo, and combinations)\n",
    "                word_embeddings: TextFieldEmbedder,\n",
    "                # same for encoders\n",
    "                encoder: Seq2VecEncoder,\n",
    "                vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.out = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('label'))\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.metrics = {\n",
    "            \"accuracy\": CategoricalAccuracy()\n",
    "        }\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self,\n",
    "                text: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # AllenNLP provides out of the box utilities for dealing with padding\n",
    "        # and also masking to exclude the padding from the computation\n",
    "        mask = get_text_field_mask(text)\n",
    "        \n",
    "        embeddings = self.word_embeddings(text)\n",
    "        # Sequence of encoded outputs\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        \n",
    "        label_logits = self.out(encoder_out)\n",
    "        output = {\"label\": label_logits}\n",
    "\n",
    "        class_probabilities = F.softmax(label_logits)\n",
    "        output_dict = {\"class_probabilities\": class_probabilities}\n",
    "\n",
    "        if label is not None:\n",
    "            loss = self.loss(label_logits, label.squeeze(-1))\n",
    "            for metric in self.metrics.values():\n",
    "                metric(label_logits, label.squeeze(-1))\n",
    "            output[\"loss\"] = loss\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data and creating the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and make vocab\n",
    "csv_reader = CSVDatasetReader()\n",
    "# create train and validation datasets\n",
    "train_dataset= csv_reader.read(data_path/'train.csv')\n",
    "validation_dataset= csv_reader.read(data_path/'validation.csv')\n",
    "# Make vocab from train and valid\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring our model\n",
    "So we said that one of the best thing of AllenNLP and the model we are building are their modularity and extensibility. Let's see how this works:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common import Params\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "word_embeddings_config = Params({\n",
    "    \"tokens\": {\n",
    "        \"type\": \"embedding\",\n",
    "        \"embedding_dim\": 50\n",
    "    }\n",
    "})\n",
    "word_embeddings = BasicTextFieldEmbedder.from_params(vocab, word_embeddings_config)\n",
    "\n",
    "encoder_config = Params({\n",
    "            \"type\": \"boe\",\n",
    "            \"embedding_dim\": 50\n",
    "})\n",
    "encoder = Seq2VecEncoder.from_params(encoder_config)\n",
    "\n",
    "# Our model gets this configured modules\n",
    "# Later we can simply change these configurations to try out new ideas\n",
    "imdb_classifier = TextClassifier(word_embeddings, encoder, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "# Using an optimizer as before\n",
    "optimizer = optim.Adam(imdb_classifier.parameters(), lr=0.1)\n",
    "\n",
    "# This handles batching for our datasets. \n",
    "# The iterator sorts instances by the specified fields in order to create \n",
    "# batches with similar sequence lengths. \n",
    "# Here we indicate that we want to sort the instances by the number of tokens in the text field\n",
    "iterator = BucketIterator(batch_size=4, sorting_keys=[(\"text\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "# Train for 10 epochs, and early stop if validation does not improve for two consec epochs\n",
    "trainer = Trainer(model=imdb_classifier,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=2,\n",
    "                  num_epochs=10)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try using pre-trained word embeddings on the embedding layer.\n",
    "\n",
    "\n",
    "2. Try adding char level tokenization and embedding.\n",
    "\n",
    "\n",
    "3. We used the simplest encoder possible, try a CNN encoder and then RNN-based ones.\n",
    "\n",
    "\n",
    "4. On the network we created before, we used two linear layers with one non-linearity\n",
    "in between, could you try here? Do you get better results? Try using the Feedforward module from allenNLP, which can be configured like this:\n",
    "```json\n",
    "\"classifier_feedforward\": {\n",
    "      \"input_dim\": 400,\n",
    "      \"num_layers\": 2,\n",
    "      \"hidden_dims\": [200, 3],\n",
    "      \"activations\": [\"relu\", \"linear\"],\n",
    "      \"dropout\": [0.2, 0.0]\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "5. Use Elmo for embedding text.\n",
    "\n",
    "\n",
    "BONUS: \n",
    "You could run the training and/or evaluation with the full-IMDB dataset available by running `fastai.untar_data(fastai.URLs.IMDB)` (hint: you will need to pre-arrange the data a little bit as its organized a bit differently, run path.ls() to see the new structure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
