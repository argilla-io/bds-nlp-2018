{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data into the network\n",
    "Modern neural nets work with what is called mini-batch gradient descent, which means that instead of feeding them one example at time, we group the examples into mini-batches. \n",
    "\n",
    "The goal now is to reuse what we've built so far to create an iterable Dataset which prepares the input data to the network.\n",
    "\n",
    "Let's recap what we had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = fastai.untar_data(fastai.URLs.IMDB_SAMPLE)\n",
    "dataset = pd.read_csv(data_path/'texts.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, lowercase=False):\n",
    "        self.lowercase = lowercase\n",
    "    def __call__(self, text):\n",
    "        return [w.lower() if self.lowercase else w for w in text.split(' ')]\n",
    "    \n",
    "class Vocab:\n",
    "    def __init__(self, unk_symbol='<unk>', is_label=False):\n",
    "        self.size = 0\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        # you will understand this later\n",
    "        if not is_label:\n",
    "            self.unk_symbol = unk_symbol\n",
    "            self.unk_id = self.add_word(unk_symbol)\n",
    "    def add_word(self, w):\n",
    "        if w not in self.word_to_id:\n",
    "            self.word_to_id[w] = self.size\n",
    "            self.id_to_word[self.size] = w\n",
    "            self.size += 1\n",
    "        return self.size - 1\n",
    "    def to_id(self, w):\n",
    "       return self.word_to_id[w] if w in self.word_to_id else self.unk_id\n",
    "    def to_word(self, id):\n",
    "       return self.id_to_word[id] if id in self.id_to_word else self.unk_symbol  \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first nn dataset\n",
    "\n",
    "Luckily, modern libraries provide simple ways to create suitable datasets. Let's just use the Dataset utility from PyTorch to create a CSVdataset, which is a simple abstract class with `__len__` and `__getitem__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CsvClassificationDataset:\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        example = { \n",
    "            'text': self.df.iloc[idx]['text'], \n",
    "            'label': self.df.iloc[idx]['label']}\n",
    "        if self.transform:\n",
    "            example = self.transform(example)\n",
    "        return example['text'], example['label']\n",
    "my_imdb_dataset = CsvClassificationDataset(data_path/'texts.csv')\n",
    "len(my_imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_imdb_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Now we can reuse the DataLoader class offered by PyTorch, which provides out of the box functions for batching examples and shuffling the dataset (an essential aspect for SGD to work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(my_imdb_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example_batch in enumerate(dataloader):\n",
    "    print(example_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have mini-batches for examples, but remember we need to turn everything into numbers and then tensors for our nn to understand the data.\n",
    "Let's do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create our text and label vocabs.\n",
    "text_tokenizer = Tokenizer(lowercase=True)\n",
    "def make_vocab(fields, is_label=False):\n",
    "    vocab = Vocab(is_label=is_label)\n",
    "    for t in fields:\n",
    "        for w in text_tokenizer(t):\n",
    "            vocab.add_word(w)\n",
    "    return vocab\n",
    "text_vocab = make_vocab(dataset['text'])\n",
    "label_vocab = make_vocab(dataset['label'], is_label=True)\n",
    "label_vocab.word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all we have left is to create our data transformation pipeline\n",
    "# which is tokenize --> numericalize (using vocab) --> to tensor\n",
    "class Tokenize:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self, sample):\n",
    "        return {'text': self.tokenizer(sample['text']),\n",
    "                'label': self.tokenizer(sample['label'])}\n",
    "\n",
    "class Numericalize:\n",
    "    def __init__(self, text_vocab, label_vocab):\n",
    "        self.text_vocab = text_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "    def _numericalize(self, toks, vocab):\n",
    "        return [vocab.to_id(w) for w in toks]\n",
    "    def __call__(self, sample):\n",
    "        return {'text': torch.tensor(self._numericalize(sample['text'], self.text_vocab)),\n",
    "                'label': torch.tensor(self._numericalize(sample['label'], self.label_vocab)[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([Tokenize(text_tokenizer),\n",
    "                                Numericalize(text_vocab, label_vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = CsvClassificationDataset(data_path/'texts.csv', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts vary in length!\n",
    "We are almost done. Except one thing: movie reviews and almost any other text have variable lenght! This is a problem for neural networks, which expect inputs (and batches) to be of a certain shape. This is another feature which makes processing texts with neural networks different to other type of inputs such as images.\n",
    "\n",
    "Without going to much into the details, we need to do what is call 'padding' which basically adds a padding token to the end of the text to reach a certain length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, dim):\n",
    "        pad_size = list(vec.shape)\n",
    "        pad_size[dim] = pad - vec.size(dim)\n",
    "        return torch.cat([vec, torch.zeros(*pad_size, dtype=torch.int64)], dim=dim)\n",
    "class PadCollate:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim \n",
    "    def pad_collate(self, batch):\n",
    "        max_len = max(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        batch = list(map(lambda b:\n",
    "                    (pad_tensor(b[0], pad=max_len, dim=self.dim), b[1]), batch))\n",
    "        xs = torch.stack(list(map(lambda x: x[0], batch)), dim=0)\n",
    "        ys = torch.LongTensor(list(map(lambda x: x[1], batch)))\n",
    "        return xs, ys\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(imdb_dataset, batch_size=16, shuffle=True, collate_fn=PadCollate(dim=0))\n",
    "for i, example in enumerate(dataloader):\n",
    "    texts, labels = example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first neural network\n",
    "\n",
    "Finally. We are ready to define and train our first neural network on text. Let's keep it as simple as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first import everything we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, n_labels):\n",
    "        super(IMDBClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.linear = nn.Linear(emb_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, n_labels)\n",
    "    def forward(self, texts):\n",
    "        embeddings = self.embedding(texts)\n",
    "        # Our input has shape `(batch_size, num_tokens, embedding_dim)`, so we sum out the `num_tokens`\n",
    "        # dimension.\n",
    "        summed = embeddings.sum(1)\n",
    "        out = self.linear(summed)\n",
    "        return self.out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IMDBClassifier(len(text_vocab), 50, 100, len(label_vocab))\n",
    "losses = []\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for i, example in enumerate(dataloader):\n",
    "        texts, labels = example\n",
    "        model.zero_grad()\n",
    "        out = model(texts)\n",
    "        loss = loss_function(out, labels)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(\"Epoch: {}.Â Loss: {}\".format(epoch, total_loss))\n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
