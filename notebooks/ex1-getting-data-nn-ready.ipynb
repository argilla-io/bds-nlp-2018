{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "We will start working with a sample from the IMDB sentiment classification dataset. The goal for now is to understand the dataset and prepare it for working with neural nets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = fastai.untar_data(fastai.URLs.IMDB_SAMPLE)\n",
    "data_path.ls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and see the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(data_path/'texts.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to one example: the sentiment `label`, input `text` and a special field added by the fastai guys to split the dataset for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "Before feeding a neural net with text, we need to turn it into sequences of numbers.\n",
    "This can be done in many ways: an integer per word, char, subword, etc. \n",
    "Let's keep it simple for now and turn words into integer ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the first example\n",
    "raw_text = dataset['text'][0]\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest tokenization we can do is splitting by white-space\n",
    "tokens = raw_text.split(' ')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have a list of words for the text. In order to turn it into integer ids, we need to build a map word -> id and viceversa (id -> word). This is what is called a vocabulary, and is an essential component of any deep learning NLP pipeline. Let's build our first vocabulary (just for this text sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set(tokens) # remove repeated tokens \n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id= {word: i for i, word in enumerate(tokens)} # turns words into ids\n",
    "this_id = word_to_id['this']\n",
    "this_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {i: word for i, word in enumerate(tokens)} # turns ids into words\n",
    "id_to_word[this_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our 'vocab' to turn our text into a sequence of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalized_tokens = [word_to_id[w] for w in tokens]\n",
    "numericalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build a vocab for the whole dataset\n",
    "all_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in dataset['text']:\n",
    "    all_tokens.extend(text.split(' '))\n",
    "all_tokens[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_all_tokens = set(all_tokens)\n",
    "# Get the size of our vocab\n",
    "len(unique_all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocab\n",
    "word_to_id = {word: i for i, word in enumerate(unique_all_tokens)}\n",
    "id_to_word = {i: word for i, word in enumerate(unique_all_tokens)}\n",
    "john_id = word_to_id['John']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word[john_id+1] # next word in the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a really simple vocab for numericalizing our training/validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning words into vectors\n",
    "We have now ids for every word. Almost every neural net for NLP uses this integer ids to get a vector for the word (or character, or..) in the first layer. This is what is know a the Embedding layer. The embedding layer is basically a lookup table of size Vxd, where V is the size of the vocab and d the dimension of the embedding vector. Let's see how this works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules import Embedding\n",
    "vocab_size = len(word_to_id)\n",
    "emb_dim = 50 \n",
    "embedding_layer = Embedding(vocab_size, emb_dim)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the vector for our first word\n",
    "v_0 = embedding_layer(torch.tensor(0)) # The network only understands torch.tensor objects\n",
    "v_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try with our first full example\n",
    "tokens = dataset['text'][0].split(' ')\n",
    "numericalized_example = [word_to_id[w] for w in tokens]\n",
    "'text of length {} tokens'.format(len(numericalized_example)), numericalized_example, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_example = embedding_layer(torch.tensor(numericalized_example))\n",
    "v_example # A matrix of the vectors corresponding to each of the 69 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_example[0] # the embedding vector of the first token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Now we have turn text into 'dense' real-valued vectors! \n",
    "Now, let's try to generalize this a little bit.\n",
    "\n",
    "But first, let's try our vocab on text outside the IMDB sample dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_movie_review_text = 'Climax from Gaspar Noé is a shockingly beatiful movie'.split(' ')\n",
    "numericalized_movie_review = [word_to_id[w] for w in my_movie_review_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of vocabulary words\n",
    "What happened?\n",
    "\n",
    "'Climax' is what it's called an out of vocabulary word (or oov, unk..). This is an important thing to deal with when working with supervised learning for NLP, as our model is expected to work with text not seen during training, validation or test. The simplest way to deal with this is to add a special token to our vocabulary which will be assigned to every unknown word. \n",
    "\n",
    "But for this let's generalize a little bit our vocabulary functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, unk_symbol='<unk>', is_label=False):\n",
    "        self.size = 1\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        # you will understand this later\n",
    "        if not is_label:\n",
    "            self.unk_symbol = unk_symbol\n",
    "            self.unk_id = self.add_word(unk_symbol)\n",
    "    def add_word(self, w):\n",
    "        if w not in self.word_to_id:\n",
    "            self.word_to_id[w] = self.size\n",
    "            self.id_to_word[self.size] = w\n",
    "            self.size += 1\n",
    "        return self.size - 1\n",
    "    def to_id(self, w):\n",
    "       return self.word_to_id[w] if w in self.word_to_id else self.unk_id\n",
    "    def to_word(self, id):\n",
    "       return self.id_to_word[id] if id in self.id_to_word else self.unk_symbol  \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "vocab = Vocab()\n",
    "vocab.to_id('Climax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.add_word('Climax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try to build the vocab for the full dataset\n",
    "full_vocab = Vocab()\n",
    "for text in dataset['text']:\n",
    "    for w in text.split(' '):\n",
    "        full_vocab.add_word(w)\n",
    "len(full_vocab) # We should get our previous lenght + 1 (for the unk token) = 36463\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let's try on our previous unseen example\n",
    "my_movie_review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalized_movie_review = [full_vocab.to_id(w) for w in my_movie_review_text]\n",
    "numericalized_movie_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalized_movie_review\n",
    "[full_vocab.to_word(i) for i in numericalized_movie_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got three unknown words, one of them a misspeling of beautiful\n",
    "# Lets try with the right spelling\n",
    "my_movie_review_text = 'Climax from Gaspar Noé is a shockingly beautiful movie'.split(' ')\n",
    "numericalized_movie_review = [full_vocab.to_id(w) for w in my_movie_review_text]\n",
    "numericalized_movie_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab.to_word(1535)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a working Vocab functionality with a simple tokenization mechanism (split by empty spaces), but what if we wanted a more general tokenizer with functions such as lowercasing, normalization, etc.?\n",
    "\n",
    "Let's try to generalize this a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, lowercase=False):\n",
    "        self.lowercase = lowercase\n",
    "    def __call__(self, text):\n",
    "        return [w.lower() if self.lowercase else w for w in text.split(' ')]\n",
    "my_tokenizer = Tokenizer(lowercase=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer('Climax is a horrible movie with nice music')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Now please build a new vocab by tokenizing the full dataset with lowercased words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try to build the vocab for the full dataset\n",
    "lowercased_vocab = Vocab()\n",
    "for text in dataset['text']:\n",
    "    # your code here\n",
    "len(lowercased_vocab) # We should get a smaller vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "We have been focusing on text, but what about labels? Labels are frequently also text, like in our case, where we have `positive`and `negative`. Neural nets don't understand text, so what should we do? We need to turn them into numbers. Good news is we can reuse our previous vocab to do this!\n",
    "\n",
    "Please create the vocab for labels. In this case we do not want the vocab to contain an unkwnow label, so we will use the is_label parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_vocab = Vocab(is_label=True)\n",
    "# Create the labels vocab\n",
    "\n",
    "# labels_vocab.word_to_id # check the generated vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! We have now almost everything we need to start training our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
